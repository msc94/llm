{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/wiki.txt\", \"r\").read()\n",
    "text = text.lower()\n",
    "# text = text[:1000]\n",
    "\n",
    "print(f\"Input has size: {len(text)}\")\n",
    "print()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(text))))\n",
    "\n",
    "stoi = dict()\n",
    "for c in chars:\n",
    "    stoi[c] = len(stoi)\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(d):\n",
    "    return \"\".join([itos[i] for i in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire dataset as a torch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print()\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2239953",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 8\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Get batch_size indices into the array\n",
    "    indices = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i + context_length] for i in indices])\n",
    "    y = torch.stack([data[i + 1:i + context_length + 1] for i in indices])\n",
    "    return x, y\n",
    "\n",
    "def print_batch_example():\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    print(xb.shape)\n",
    "    print(xb)\n",
    "    print(yb.shape)\n",
    "    print(yb)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c in range(context_length):\n",
    "            context = xb[b, :c + 1]\n",
    "            targets = yb[b, c]\n",
    "            print(f\"{context} -> {targets}\")\n",
    "\n",
    "print_batch_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3225d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, context, targets=None):\n",
    "        # idx is (batch_size, context_length)\n",
    "        logits = self.embedding(context) # (batch_size, context_length, embedding_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, context_length, embedding_size = logits.shape\n",
    "            \n",
    "            logits = logits.view(batch_size * context_length, embedding_size)\n",
    "            targets = targets.view(batch_size * context_length)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context, num_new_tokens):\n",
    "        for _ in range(num_new_tokens):\n",
    "            # Predict next token\n",
    "            logits, _ = self(context)\n",
    "            # Get the raw outputs for the next token\n",
    "            logits = logits[:, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            # Get next token from probabilities\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            # Append next token to context\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "        return context\n",
    "            \n",
    "            \n",
    "    \n",
    "m = Model(vocab_size)\n",
    "xb, yb = get_batch(\"train\")\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bde478",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    m.eval()\n",
    "    \n",
    "    num_batches = 100\n",
    "    split_losses = dict()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(num_batches)\n",
    "        for i in range(num_batches):\n",
    "            # Sample batch\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = m(xb, yb)\n",
    "            losses[i] = loss\n",
    "        split_losses[split] = losses.mean().item()\n",
    "        \n",
    "    m.train()\n",
    "    \n",
    "    return split_losses[\"train\"], split_losses[\"val\"]\n",
    "    \n",
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_iterations = 100_000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iteration in (pbar := tqdm.tqdm(range(num_iterations))):\n",
    "    if iteration % 1000 == 0:\n",
    "        train_loss, val_loss = estimate_losses()\n",
    "        pbar.set_description(f\"T: {train_loss}, V: {val_loss}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    # Sample batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Backwards pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the model generate something\n",
    "raw_output = m.generate(torch.zeros(1, 1, dtype=torch.long), 1000)\n",
    "print(decode(raw_output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
