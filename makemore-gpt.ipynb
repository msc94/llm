{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69d181",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d69d181",
    "outputId": "46d1d6ea-22fb-477e-cfca-1c86dbeb00a0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037cc66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0037cc66",
    "outputId": "22c27b9a-86c4-48de-d18e-c7b7c5d46176"
   },
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "context_length = 8\n",
    "embedding_size = 32\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "\n",
    "num_iterations = 10_000\n",
    "learning_rate = 1e-3\n",
    "use_subset = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b668a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "941b668a",
    "outputId": "e42c02f5-d58c-4444-c9ef-fdacac0d386c"
   },
   "outputs": [],
   "source": [
    "text = open(\"data/wiki.txt\", \"r\").read()\n",
    "text = text.lower()\n",
    "\n",
    "if use_subset:\n",
    "    text = text[:1000]\n",
    "\n",
    "print(f\"Input has size: {len(text)}\")\n",
    "print()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251ca63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f251ca63",
    "outputId": "8fd9ce56-d2f7-4c56-d678-7f1f61e7685f"
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(\"\".join(text))))\n",
    "\n",
    "stoi = dict()\n",
    "for c in chars:\n",
    "    stoi[c] = len(stoi)\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(d):\n",
    "    return \"\".join([itos[i] for i in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f46e2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35f46e2f",
    "outputId": "298a415d-845d-44b3-870b-29d17591f958"
   },
   "outputs": [],
   "source": [
    "# Encode the entire dataset as a torch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long).to(device)\n",
    "print(data.shape)\n",
    "print()\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d238d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77d238d5",
    "outputId": "493abc22-b14b-4217-b0e8-1211c35e79e1"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a9a6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a32a9a6e",
    "outputId": "32c2cb7c-1b6e-49f8-a5d6-18b185653fb2"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Get batch_size indices into the array\n",
    "    indices = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i + context_length] for i in indices])\n",
    "    y = torch.stack([data[i + 1:i + context_length + 1] for i in indices])\n",
    "    return x, y\n",
    "\n",
    "def print_batch_example():\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    print(xb.shape)\n",
    "    print(xb)\n",
    "    print(yb.shape)\n",
    "    print(yb)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c in range(context_length):\n",
    "            context = xb[b, :c + 1]\n",
    "            targets = yb[b, c]\n",
    "            print(f\"{context} -> {targets}\")\n",
    "\n",
    "print_batch_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3225d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb3225d4",
    "outputId": "a1063353-949d-438d-a22e-743550e8d424"
   },
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.K = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.Q = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.V = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Triangular matrix that makes sure that tokens cannot look at following tokens\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        k = self.K(x) # (B, T, head_size)\n",
    "        q = self.K(x) # (B, T, head_size)\n",
    "        \n",
    "        # Compute attention\n",
    "        wei = q @ torch.transpose(k, -1, -2) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "        wei = wei / C**0.5\n",
    "        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = torch.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.V(x) # (B, T, head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(size, 4 * size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * size, size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffwd = FeedForward(embedding_size)\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # With residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, embedding_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embedding_size, num_heads=num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "    def forward(self, context, targets=None):\n",
    "        B, T = context.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(context) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        logits = self.lm_head(self.ln(x)) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context, num_new_tokens):\n",
    "        # context is (B, T)\n",
    "        for _ in range(num_new_tokens):\n",
    "            # Crop context to only contain context_length tokens\n",
    "            cropped_context = context[:, -context_length:]\n",
    "            # Predict next token\n",
    "            logits, _ = self(cropped_context) # (B, T, C)\n",
    "            # Get the raw outputs for the next token\n",
    "            logits = logits[:, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            # Get next token from probabilities\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            # Append next token to context\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "        return context\n",
    "            \n",
    "            \n",
    "    \n",
    "m = Model().to(device)\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1e2b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fec1e2b7",
    "outputId": "f9f076b9-5ca1-4fb1-a56a-cf6eac7a43c1"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    m.eval()\n",
    "    \n",
    "    num_batches = 10\n",
    "    split_losses = dict()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(num_batches)\n",
    "        for i in range(num_batches):\n",
    "            # Sample batch\n",
    "            xb, yb = get_batch(split)\n",
    "            _, loss = m(xb, yb)\n",
    "            losses[i] = loss\n",
    "        split_losses[split] = losses.mean().item()\n",
    "        \n",
    "    m.train()\n",
    "    \n",
    "    return split_losses[\"train\"], split_losses[\"val\"]\n",
    "    \n",
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8dda6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af8dda6d",
    "outputId": "10d046fc-647b-43d2-fd3c-7456460cf20a"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "for iteration in (pbar := tqdm.tqdm(range(num_iterations))):\n",
    "    if iteration % 200 == 0:\n",
    "        train_loss, val_loss = estimate_loss()\n",
    "        pbar.set_description(f\"T: {train_loss:.3}, V: {val_loss:.3}\")\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    # Sample batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Backwards pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04297987",
   "metadata": {
    "id": "04297987"
   },
   "outputs": [],
   "source": [
    "# Let the model generate something\n",
    "raw_output = m.generate(torch.zeros(1, 1, dtype=torch.long), 1000)\n",
    "print(decode(raw_output[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefa406",
   "metadata": {
    "id": "7eefa406"
   },
   "source": [
    "## History\n",
    "\n",
    "```\n",
    "Bigram:      T: 2.48, V: 2.48\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
